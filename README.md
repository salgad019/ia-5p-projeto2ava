# ğŸ· IA-projeto8: AnÃ¡lise Comparativa de ReduÃ§Ã£o de Dimensionalidade com PCA

## ğŸ“‹ DescriÃ§Ã£o do Projeto

Este projeto implementa uma **anÃ¡lise comparativa completa** do impacto da reduÃ§Ã£o de dimensionalidade usando **PCA (Principal Component Analysis)** em algoritmos de machine learning. O estudo avalia como diferentes nÃ­veis de compressÃ£o dimensional afetam a performance, velocidade de treinamento e inferÃªncia em mÃºltiplos datasets.

## ğŸ¯ Objetivos

- **Comparar performance** de algoritmos ML com e sem reduÃ§Ã£o dimensional
- **Analisar trade-offs** entre acurÃ¡cia e eficiÃªncia computacional
- **Visualizar** como PCA preserva/perde informaÃ§Ã£o dos dados
- **Avaliar** impacto da dimensionalidade em diferentes tipos de datasets

## ğŸ“Š Datasets Utilizados

### 1. **Wine Dataset (UCI)** ğŸ¾
- **Amostras:** 178
- **Features:** 13 caracterÃ­sticas quÃ­micas
- **Classes:** 3 tipos de vinho
- **Tipo:** Dados reais de anÃ¡lise quÃ­mica

### 2. **Diabetes Dataset** ğŸ¥
- **Amostras:** 100.000 (sintÃ©tico)
- **Features:** 20 caracterÃ­sticas mÃ©dicas
- **Classes:** 2 (diabetes/nÃ£o-diabetes)
- **Tipo:** Dados sintÃ©ticos simulando caracterÃ­sticas mÃ©dicas

### 3. **Soybean Seeds Dataset** ğŸŒ±
- **Amostras:** 30.000 (sintÃ©tico)
- **Features:** 35 caracterÃ­sticas morfolÃ³gicas
- **Classes:** 19 variedades de sementes
- **Tipo:** Dados sintÃ©ticos simulando caracterÃ­sticas de sementes

## ğŸ”¬ Metodologia

### 1. **PrÃ©-processamento**
- NormalizaÃ§Ã£o dos dados (StandardScaler)
- DivisÃ£o treino/teste (70%/30%)
- EstratificaÃ§Ã£o para manter proporÃ§Ã£o das classes

### 2. **AnÃ¡lise PCA**
- **PCA 90%:** Componentes necessÃ¡rios para 90% da variÃ¢ncia
- **PCA 2D:** ReduÃ§Ã£o para 2 componentes principais
- **PCA 3D:** ReduÃ§Ã£o para 3 componentes principais
- **VisualizaÃ§Ã£o:** Curva de variÃ¢ncia acumulada

### 3. **Algoritmos Testados**
- **k-NN (k=5):** SensÃ­vel Ã  dimensionalidade
- **RegressÃ£o LogÃ­stica:** Menos sensÃ­vel Ã  dimensionalidade

### 4. **MÃ©tricas Avaliadas**
- âœ… **AcurÃ¡cia:** Percentual de prediÃ§Ãµes corretas
- â±ï¸ **Tempo de Treinamento:** Velocidade de ajuste do modelo
- ğŸš€ **Tempo de InferÃªncia:** Velocidade de prediÃ§Ã£o

## ğŸ“ˆ Experimentos Realizados

Cada dataset Ã© testado em **6 configuraÃ§Ãµes diferentes**:

| Dataset | Abordagem | Modelo | DimensÃµes |
|---------|-----------|--------|-----------|
| Wine/Diabetes/Soybean | Original | k-NN | 13/20/35 |
| Wine/Diabetes/Soybean | PCA-2D | k-NN | 2 |
| Wine/Diabetes/Soybean | PCA-3D | k-NN | 3 |
| Wine/Diabetes/Soybean | Original | LogReg | 13/20/35 |
| Wine/Diabetes/Soybean | PCA-2D | LogReg | 2 |
| Wine/Diabetes/Soybean | PCA-3D | LogReg | 3 |

**Total:** 18 experimentos (3 datasets Ã— 6 configuraÃ§Ãµes)

## ğŸ“Š VisualizaÃ§Ãµes Geradas

1. **ğŸ“ˆ Curva de VariÃ¢ncia Acumulada** - Mostra quantos componentes preservam X% da informaÃ§Ã£o
2. **ğŸ¨ Scatter Plot 2D** - Visualiza separaÃ§Ã£o das classes no espaÃ§o reduzido
3. **ğŸ“Š DistribuiÃ§Ã£o Original** - VisualizaÃ§Ã£o dos dados antes do processamento
4. **âš¡ ComparaÃ§Ã£o de AcurÃ¡cia** - Performance por dataset e abordagem
5. **â±ï¸ Tempo de Treinamento** - EficiÃªncia computacional no treino
6. **ğŸš€ Tempo de InferÃªncia** - Velocidade de prediÃ§Ã£o
7. **ğŸ“¦ DistribuiÃ§Ã£o por Abordagem** - Box plots comparativos

## ğŸš€ Como Executar

### PrÃ©-requisitos
```bash
pip install scikit-learn pandas numpy matplotlib seaborn
```

### ExecuÃ§Ã£o
```bash
python projeto.py
```

## ğŸ“‹ Estrutura do CÃ³digo

```python
â”œâ”€â”€ FunÃ§Ãµes de Carregamento de Datasets
â”‚   â”œâ”€â”€ carregar_diabetes_dataset()
â”‚   â””â”€â”€ carregar_soybean_dataset()
â”œâ”€â”€ Processamento e AnÃ¡lise PCA
â”‚   â””â”€â”€ processar_dataset()
â”œâ”€â”€ AvaliaÃ§Ã£o de Modelos
â”‚   â”œâ”€â”€ avaliar_modelo()
â”‚   â””â”€â”€ executar_experimentos()
â””â”€â”€ VisualizaÃ§Ãµes e Resultados
    â””â”€â”€ GrÃ¡ficos comparativos
```

## ğŸ“Š Resultados Esperados

### ğŸ”´ **k-NN (SensÃ­vel Ã  Dimensionalidade)**
- **Dados Originais:** Alta acurÃ¡cia, treino/inferÃªncia lentos
- **PCA 2D/3D:** PossÃ­vel queda na acurÃ¡cia, mas muito mais rÃ¡pido

### ğŸ”µ **RegressÃ£o LogÃ­stica (Robusta)**
- **Dados Originais:** Boa acurÃ¡cia, treino moderado
- **PCA 2D/3D:** Pouca perda de acurÃ¡cia, treino mais rÃ¡pido

### ğŸ“ˆ **Trade-offs Identificados**
- âš–ï¸ **AcurÃ¡cia vs Velocidade:** ReduÃ§Ã£o dimensional acelera, mas pode perder precisÃ£o
- ğŸ” **Interpretabilidade vs Performance:** PCA perde interpretabilidade das features originais
- ğŸ’¾ **MemÃ³ria vs Qualidade:** Menos dimensÃµes = menos memÃ³ria, possÃ­vel perda de informaÃ§Ã£o

## ğŸ“ Conceitos Demonstrados

- **ğŸ¯ MaldiÃ§Ã£o da Dimensionalidade:** Como excesso de dimensÃµes pode prejudicar algoritmos
- **âš¡ ReduÃ§Ã£o de Dimensionalidade:** TÃ©cnicas para compressar dados mantendo informaÃ§Ã£o
- **ğŸ“Š AnÃ¡lise de Componentes Principais:** Como PCA funciona e seus trade-offs
- **ğŸ”„ AvaliaÃ§Ã£o SistemÃ¡tica:** ImportÃ¢ncia de testar mÃºltiplas configuraÃ§Ãµes
- **ğŸ“ˆ VisualizaÃ§Ã£o de Dados:** Como "ver" dados multidimensionais

## ğŸ† AplicaÃ§Ãµes PrÃ¡ticas

- **ğŸ” AnÃ¡lise ExploratÃ³ria:** Visualizar dados de alta dimensÃ£o
- **âš¡ OtimizaÃ§Ã£o de Performance:** Acelerar algoritmos em produÃ§Ã£o
- **ğŸ’¾ CompressÃ£o de Dados:** Reduzir espaÃ§o de armazenamento
- **ğŸ¯ Feature Engineering:** Identificar caracterÃ­sticas mais importantes
- **ğŸš€ Sistemas em Tempo Real:** Reduzir latÃªncia de prediÃ§Ãµes

## ğŸ“š Tecnologias Utilizadas

- **ğŸ Python 3.7+**
- **ğŸ¤– scikit-learn:** Algoritmos ML e PCA
- **ğŸ“Š pandas:** ManipulaÃ§Ã£o de dados
- **ğŸ”¢ numpy:** OperaÃ§Ãµes numÃ©ricas
- **ğŸ“ˆ matplotlib/seaborn:** VisualizaÃ§Ãµes


---

*Este projeto demonstra na prÃ¡tica como a reduÃ§Ã£o de dimensionalidade pode ser uma ferramenta poderosa para otimizar algoritmos de machine learning, fornecendo insights valiosos sobre o trade-off entre performance e eficiÃªncia computacional.*
